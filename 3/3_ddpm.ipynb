{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RostislavKorst/Generative-Models-2024/blob/main/Assignment%205/5_ddpm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKPGLKEJLnXg"
      },
      "source": [
        "# Generative Models\n",
        "***\n",
        "\n",
        "**Autumn 2024**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FJykNlbL6b4"
      },
      "source": [
        "## Assignment 3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-pnnVDjJjfX"
      },
      "source": [
        "In this assignment we implement DDPM - Denoising Diffusion Probabilistic Models (2020)\n",
        "\n",
        "In simple terms, we get an image from data and add noise step by step. Then We train a model to predict that noise at each step and use the model to generate images.\n",
        "\n",
        "The following definitions and derivations show how this works. For details please refer to the paper https://arxiv.org/abs/2006.11239"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Plx0nFzU3G5"
      },
      "source": [
        "## Forward Process\n",
        "\n",
        "The forward process adds noise to the data $x_0 \\sim q(x_0)$, for $T$ timesteps.\n",
        "\n",
        "\\begin{align}\n",
        "q(x_t | x_{t-1}) = \\mathcal{N}\\big(x_t; \\sqrt{1-  \\beta_t} x_{t-1}, \\beta_t \\mathbf{I}\\big) \\\\\n",
        "q(x_{1:T} | x_0) = \\prod_{t = 1}^{T} q(x_t | x_{t-1})\n",
        "\\end{align}\n",
        "\n",
        "where $\\beta_1, \\dots, \\beta_T$ is the variance schedule.\n",
        "\n",
        "We can sample $x_t$ at any timestep $t$ with,\n",
        "\n",
        "\\begin{align}\n",
        "q(x_t|x_0) &= \\mathcal{N} \\Big(x_t; \\sqrt{\\bar\\alpha_t} x_0, (1-\\bar\\alpha_t) \\mathbf{I} \\Big)\n",
        "\\end{align}\n",
        "\n",
        "where $\\alpha_t = 1 - \\beta_t$ and $\\bar\\alpha_t = \\prod_{s=1}^t \\alpha_s$\n",
        "\n",
        "## Reverse Process\n",
        "\n",
        "The reverse process removes noise starting at $p(x_T) = \\mathcal{N}(x_T; \\mathbf{0}, \\mathbf{I})$\n",
        "for $T$ time steps.\n",
        "\n",
        "\\begin{align}\n",
        "p_\\theta(x_{t-1} | x_t) &= \\mathcal{N}\\big(x_{t-1};\n",
        " \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t)\\big) \\\\\n",
        "p_\\theta(x_{0:T}) &= p_\\theta(x_T) \\prod_{t = 1}^{T} p_\\theta(x_{t-1} | x_t) \\\\\n",
        "p_\\theta(x_0) &= \\int p_\\theta(x_{0:T}) dx_{1:T}\n",
        "\\end{align}\n",
        "\n",
        "$\\theta$ are the parameters we train.\n",
        "\n",
        "## Loss\n",
        "\n",
        "We optimize the ELBO (from Jenson's inequality) on the negative log likelihood.\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbb{E}[-\\log p_\\theta(x_0)]\n",
        " &\\le \\mathbb{E}_q [ -\\log \\frac{p_\\theta(x_{0:T})}{q(x_{1:T}|x_0)} ] \\\\\n",
        " &=L\n",
        "\\end{align}\n",
        "\n",
        "The loss can be rewritten as  follows.\n",
        "\n",
        "\\begin{align}\n",
        "L\n",
        " &= \\mathbb{E}_q [ -\\log \\frac{p_\\theta(x_{0:T})}{q(x_{1:T}|x_0)} ] \\\\\n",
        " &= \\mathbb{E}_q [ -\\log p(x_T) - \\sum_{t=1}^T \\log \\frac{p_\\theta(x_{t-1}|x_t)}{q(x_t|x_{t-1})} ] \\\\\n",
        " &= \\mathbb{E}_q [\n",
        "  -\\log \\frac{p(x_T)}{q(x_T|x_0)}\n",
        "  -\\sum_{t=2}^T \\log \\frac{p_\\theta(x_{t-1}|x_t)}{q(x_{t-1}|x_t,x_0)}\n",
        "  -\\log p_\\theta(x_0|x_1)] \\\\\n",
        " &= \\mathbb{E}_q [\n",
        "   D_{KL}(q(x_T|x_0) \\Vert p(x_T))\n",
        "  +\\sum_{t=2}^T D_{KL}(q(x_{t-1}|x_t,x_0) \\Vert p_\\theta(x_{t-1}|x_t))\n",
        "  -\\log p_\\theta(x_0|x_1)]\n",
        "\\end{align}\n",
        "\n",
        "$D_{KL}(q(x_T|x_0) \\Vert p(x_T))$ is constant since we keep $\\beta_1, \\dots, \\beta_T$ constant.\n",
        "\n",
        "### Computing $L_{t-1} = D_{KL}(q(x_{t-1}|x_t,x_0) \\Vert p_\\theta(x_{t-1}|x_t))$\n",
        "\n",
        "The forward process posterior conditioned by $x_0$ is,\n",
        "\n",
        "\\begin{align}\n",
        "q(x_{t-1}|x_t, x_0) &= \\mathcal{N} \\Big(x_{t-1}; \\tilde\\mu_t(x_t, x_0), \\tilde\\beta_t \\mathbf{I} \\Big) \\\\\n",
        "\\tilde\\mu_t(x_t, x_0) &= \\frac{\\sqrt{\\bar\\alpha_{t-1}}\\beta_t}{1 - \\bar\\alpha_t}x_0\n",
        "                         + \\frac{\\sqrt{\\alpha_t}(1 - \\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}x_t \\\\\n",
        "\\tilde\\beta_t &= \\frac{1 - \\bar\\alpha_{t-1}}{1 - \\bar\\alpha_t} \\beta_t\n",
        "\\end{align}\n",
        "\n",
        "The paper sets $\\Sigma_\\theta(x_t, t) = \\sigma_t^2 \\mathbf{I}$ where $\\sigma_t^2$ is set to constants\n",
        "$\\beta_t$ or $\\tilde\\beta_t$.\n",
        "\n",
        "Then,\n",
        "$$p_\\theta(x_{t-1} | x_t) = \\mathcal{N}\\big(x_{t-1}; \\mu_\\theta(x_t, t), \\sigma_t^2 \\mathbf{I} \\big)$$\n",
        "\n",
        "For given noise $\\epsilon \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ using $q(x_t|x_0)$\n",
        "\n",
        "\\begin{align}\n",
        "x_t(x_0, \\epsilon) &= \\sqrt{\\bar\\alpha_t} x_0 + \\sqrt{1-\\bar\\alpha_t}\\epsilon \\\\\n",
        "x_0 &= \\frac{1}{\\sqrt{\\bar\\alpha_t}} \\Big(x_t(x_0, \\epsilon) -  \\sqrt{1-\\bar\\alpha_t}\\epsilon\\Big)\n",
        "\\end{align}\n",
        "\n",
        "This gives,\n",
        "\n",
        "\\begin{align}\n",
        "L_{t-1}\n",
        " &= D_{KL}(q(x_{t-1}|x_t,x_0) \\Vert p_\\theta(x_{t-1}|x_t)) \\\\\n",
        " &= \\mathbb{E}_q \\Bigg[ \\frac{1}{2\\sigma_t^2}\n",
        " \\Big \\Vert \\tilde\\mu(x_t, x_0) - \\mu_\\theta(x_t, t) \\Big \\Vert^2 \\Bigg] \\\\\n",
        " &= \\mathbb{E}_{x_0, \\epsilon} \\Bigg[ \\frac{1}{2\\sigma_t^2}\n",
        "  \\bigg\\Vert \\frac{1}{\\sqrt{\\alpha_t}} \\Big(\n",
        "  x_t(x_0, \\epsilon) - \\frac{\\beta_t}{\\sqrt{1 - \\bar\\alpha_t}} \\epsilon\n",
        "  \\Big) - \\mu_\\theta(x_t(x_0, \\epsilon), t) \\bigg\\Vert^2 \\Bigg] \\\\\n",
        "\\end{align}\n",
        "\n",
        "Re-parameterizing with a model to predict noise\n",
        "\n",
        "\\begin{align}\n",
        "\\mu_\\theta(x_t, t) &= \\tilde\\mu \\bigg(x_t,\n",
        "  \\frac{1}{\\sqrt{\\bar\\alpha_t}} \\Big(x_t -\n",
        "   \\sqrt{1-\\bar\\alpha_t}\\epsilon_\\theta(x_t, t) \\Big) \\bigg) \\\\\n",
        "  &= \\frac{1}{\\sqrt{\\alpha_t}} \\Big(x_t -\n",
        "  \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\epsilon_\\theta(x_t, t) \\Big)\n",
        "\\end{align}\n",
        "\n",
        "where $\\epsilon_\\theta$ is a learned function that predicts $\\epsilon$ given $(x_t, t)$.\n",
        "\n",
        "This gives,\n",
        "\n",
        "\\begin{align}\n",
        "L_{t-1}\n",
        "&= \\mathbb{E}_{x_0, \\epsilon} \\Bigg[ \\frac{\\beta_t^2}{2\\sigma_t^2 \\alpha_t (1 - \\bar\\alpha_t)}\n",
        "  \\Big\\Vert\n",
        "  \\epsilon - \\epsilon_\\theta(\\sqrt{\\bar\\alpha_t} x_0 + \\sqrt{1-\\bar\\alpha_t}\\epsilon, t)\n",
        "  \\Big\\Vert^2 \\Bigg]\n",
        "\\end{align}\n",
        "\n",
        "That is, we are training to predict the noise.\n",
        "\n",
        "### Simplified loss\n",
        "\n",
        "$$L_{\\text{simple}}(\\theta) = \\mathbb{E}_{t,x_0, \\epsilon} \\Bigg[ \\bigg\\Vert\n",
        "\\epsilon - \\epsilon_\\theta(\\sqrt{\\bar\\alpha_t} x_0 + \\sqrt{1-\\bar\\alpha_t}\\epsilon, t)\n",
        "\\bigg\\Vert^2 \\Bigg]$$\n",
        "\n",
        "This minimizes $-\\log p_\\theta(x_0|x_1)$ when $t=1$ and $L_{t-1}$ for $t\\gt1$ discarding the\n",
        "weighting in $L_{t-1}$. Discarding the weights $\\frac{\\beta_t^2}{2\\sigma_t^2 \\alpha_t (1 - \\bar\\alpha_t)}$\n",
        "increase the weight given to higher $t$ (which have higher noise levels), therefore increasing the sample quality.\n",
        "\n",
        "This file implements the loss calculation and a basic sampling method that we use to generate images during\n",
        "training.\n",
        "\n",
        "$\\epsilon_\\theta(x_t, t)$ is a UNet model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "gUlR_dqfzB_O"
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "from torch import nn\n",
        "import torchvision\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "N9nuTxORyyRK"
      },
      "outputs": [],
      "source": [
        "# Load file unet.py to the filesystem of colab\n",
        "from unet import UNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "bqvnCO8q1Oug"
      },
      "outputs": [],
      "source": [
        "def gather(consts: torch.Tensor, t: torch.Tensor):\n",
        "    \"\"\"Gather consts for t and reshape to feature map shape\"\"\"\n",
        "    c = consts.gather(-1, t)\n",
        "    return c.reshape(-1, 1, 1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pxv4piCzTzf"
      },
      "source": [
        "### Explanations to the code\n",
        "[1] $\\bar\\alpha_t = \\prod_{s=1}^t \\alpha_s$\n",
        "\n",
        "[2] \\begin{align}\n",
        "        q(x_t|x_0) &= \\mathcal{N} \\Big(x_t; \\sqrt{\\bar\\alpha_t} x_0, (1-\\bar\\alpha_t) \\mathbf{I} \\Big)\n",
        "        \\end{align}\n",
        "\n",
        "[3] gather $\\alpha_t$ and compute $\\sqrt{\\bar\\alpha_t} x_0$\n",
        "\n",
        "[4] $(1-\\bar\\alpha_t) \\mathbf{I}$\n",
        "\n",
        "[5] \\begin{align}\n",
        "        q(x_t|x_0) &= \\mathcal{N} \\Big(x_t; \\sqrt{\\bar\\alpha_t} x_0, (1-\\bar\\alpha_t) \\mathbf{I} \\Big)\n",
        "        \\end{align}\n",
        "\n",
        "[6] $\\epsilon \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$\n",
        "\n",
        "[7] \\begin{align}\n",
        "        p_\\theta(x_{t-1} | x_t) &= \\mathcal{N}\\big(x_{t-1};\n",
        "        \\mu_\\theta(x_t, t), \\sigma_t^2 \\mathbf{I} \\big) \\\\\n",
        "        \\mu_\\theta(x_t, t)\n",
        "          &= \\frac{1}{\\sqrt{\\alpha_t}} \\Big(x_t -\n",
        "            \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\epsilon_\\theta(x_t, t) \\Big)\n",
        "        \\end{align}\n",
        "\n",
        "[8] gather $\\bar\\alpha_t$\n",
        "\n",
        "[9] $\\frac{\\beta}{\\sqrt{1-\\bar\\alpha_t}}$\n",
        "\n",
        "[10] $$\\frac{1}{\\sqrt{\\alpha_t}} \\Big(x_t -\n",
        "              \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\epsilon_\\theta(x_t, t) \\Big)$$\n",
        "\n",
        "[11] $\\epsilon \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$\n",
        "\n",
        "[12] $$L_{\\text{simple}}(\\theta) = \\mathbb{E}_{t,x_0, \\epsilon} \\Bigg[ \\bigg\\Vert\n",
        "        \\epsilon - \\epsilon_\\theta(\\sqrt{\\bar\\alpha_t} x_0 + \\sqrt{1-\\bar\\alpha_t}\\epsilon, t)\n",
        "        \\bigg\\Vert^2 \\Bigg]$$\n",
        "\n",
        "[13] $\\epsilon \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$\n",
        "\n",
        "[14] Get $\\epsilon_\\theta(\\sqrt{\\bar\\alpha_t} x_0 + \\sqrt{1-\\bar\\alpha_t}\\epsilon, t)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "zhrbZ3dIzd5-"
      },
      "outputs": [],
      "source": [
        "class DenoiseDiffusion:\n",
        "    \"\"\"\n",
        "    ## Denoise Diffusion\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, eps_model: nn.Module, n_steps: int, device: torch.device):\n",
        "        \"\"\"\n",
        "        * eps_model - epsilon_theta(x_t, t) model\n",
        "        * n_steps - t\n",
        "        * device - the device to place constants on\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.eps_model = eps_model\n",
        "\n",
        "        # Create beta_1 ... beta_T linearly increasing variance schedule\n",
        "        self.beta = torch.linspace(0.0001, 0.02, n_steps).to(device)\n",
        "\n",
        "        # alpha_t = 1 - beta_t\n",
        "        self.alpha = 1.0 - self.beta\n",
        "        # [1]\n",
        "        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n",
        "        # T\n",
        "        self.n_steps = n_steps\n",
        "        # sigma^2 = beta\n",
        "        self.sigma2 = self.beta\n",
        "\n",
        "    def q_xt_x0(self, x0: torch.Tensor, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Get q(x_t|x_0) distribution\n",
        "\n",
        "        [2]\n",
        "        \"\"\"\n",
        "        # [3]\n",
        "        mean = gather(self.alpha_bar, t) ** 0.5 * x0\n",
        "        # [4]\n",
        "        var = 1 - gather(self.alpha_bar, t)\n",
        "        return mean, var\n",
        "\n",
        "    def q_sample(self, x0: torch.Tensor, t: torch.Tensor, eps: Optional[torch.Tensor] = None):\n",
        "        \"\"\"\n",
        "        Sample from q(x_t|x_0)\n",
        "\n",
        "        [5]\n",
        "        \"\"\"\n",
        "\n",
        "        # [6]\n",
        "        if eps is None:\n",
        "            eps = torch.randn_like(x0)\n",
        "\n",
        "        # get q(x_t|x_0)\n",
        "        mean, var = self.q_xt_x0(x0, t)\n",
        "        # Sample from q(x_t|x_0)\n",
        "        return mean + (var ** 0.5) * eps\n",
        "\n",
        "    def p_sample(self, xt: torch.Tensor, t: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Sample from p_theta(x_{t-1}|x_t)\n",
        "\n",
        "        [7]\n",
        "        \"\"\"\n",
        "\n",
        "        # epsilon_theta(x_t, t)\n",
        "        eps_theta = self.eps_model(xt, t)\n",
        "        # [8]\n",
        "        alpha_bar = gather(self.alpha_bar, t)\n",
        "        # alpha_t\n",
        "        alpha = self.alpha[t]\n",
        "        # [9]\n",
        "        eps_coef = self.beta[t] / torch.sqrt(1 - alpha_bar)\n",
        "        # [10]\n",
        "        mean = 1 / torch.sqrt(alpha) * (xt - eps_coef * eps_theta)\n",
        "        # sigma^2\n",
        "        var = gather(self.sigma2, t)\n",
        "\n",
        "        # [11]\n",
        "        eps = torch.randn_like(xt)\n",
        "        # Sample\n",
        "        return mean + (var ** .5) * eps\n",
        "\n",
        "    def loss(self, x0: torch.Tensor, noise: Optional[torch.Tensor] = None):\n",
        "        \"\"\"\n",
        "        Simplified Loss\n",
        "\n",
        "        [12]\n",
        "        \"\"\"\n",
        "        # Get batch size\n",
        "        batch_size = x0.size(0)\n",
        "        # Get random t for each sample in the batch\n",
        "        t = torch.randint(0, self.n_steps, (batch_size,), device=x0.device)\n",
        "\n",
        "        # [13]\n",
        "        if noise is None:\n",
        "            noise = torch.randn_like(x0)\n",
        "\n",
        "        # Sample x_t for q(x_t|x_0)\n",
        "        xt = self.q_sample(x0, t, noise)\n",
        "        # [14]\n",
        "        eps_theta = self.eps_model(xt, t)\n",
        "\n",
        "        # MSE loss\n",
        "        return torch.mean((eps_theta - noise) ** 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktW42v0L5pHv"
      },
      "source": [
        "### Explanations to the code\n",
        "[1] $x_T \\sim p(x_T) = \\mathcal{N}(x_T; \\mathbf{0}, \\mathbf{I})$\n",
        "\n",
        "[2] Sample from $p_\\theta(x_{t-1}|x_t)$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "01JVUUrEz0Am",
        "outputId": "5f135a92-ac44-476c-81c5-3c11f9e7a874"
      },
      "outputs": [],
      "source": [
        "class MNISTDataset(torchvision.datasets.MNIST):\n",
        "    def __init__(self, image_size):\n",
        "        transform = torchvision.transforms.Compose([\n",
        "            torchvision.transforms.Resize(image_size),\n",
        "            torchvision.transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "        super().__init__(\"data\", train=True, download=True, transform=transform)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return super().__getitem__(item)[0]\n",
        "\n",
        "def plot_samples(tensor):\n",
        "    # Assuming you have a tensor of size torch.Size([16, 1, 32, 32])\n",
        "    # Convert the tensor to a numpy array\n",
        "    images = tensor.numpy()\n",
        "\n",
        "    # Reshape the images to be of size (16, 32, 32)\n",
        "    images = np.reshape(images, (16, 32, 32))\n",
        "\n",
        "    # Create a figure with a grid of subplots\n",
        "    fig, axes = plt.subplots(nrows=4, ncols=4)\n",
        "\n",
        "    # Iterate over the images and plot them on the subplots\n",
        "    for i, ax in enumerate(axes.flatten()):\n",
        "        ax.imshow(images[i], cmap='gray')\n",
        "        ax.axis('off')\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "class Configs:\n",
        "    device: torch.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # U-Net model for epsilon_theta(x_t, t)\n",
        "    eps_model: UNet\n",
        "    # DDPM algorithm\n",
        "    diffusion: DenoiseDiffusion\n",
        "\n",
        "    # Number of channels in the image. 3 for RGB.\n",
        "    image_channels: int = 1\n",
        "    # Image size\n",
        "    image_size: int = 32\n",
        "    # Number of channels in the initial feature map\n",
        "    n_channels: int = 64\n",
        "    # The list of channel numbers at each resolution.\n",
        "    # The number of channels is `channel_multipliers[i] * n_channels`\n",
        "    channel_multipliers: List[int] = [1, 2, 2, 4]\n",
        "    # The list of booleans that indicate whether to use attention at each resolution\n",
        "    is_attention: List[int] = [False, False, False, True]\n",
        "\n",
        "    # Number of time steps T\n",
        "    n_steps: int = 1_000\n",
        "    # Batch size\n",
        "    batch_size: int = 64\n",
        "    # Number of samples to generate\n",
        "    n_samples: int = 16\n",
        "    # Learning rate\n",
        "    learning_rate: float = 2e-5\n",
        "\n",
        "    # Number of training epochs\n",
        "    epochs: int = 5\n",
        "\n",
        "    # Dataset\n",
        "    dataset: torch.utils.data.Dataset = MNISTDataset(image_size)\n",
        "    # Dataloader\n",
        "    data_loader: torch.utils.data.DataLoader\n",
        "\n",
        "    # Adam optimizer\n",
        "    optimizer: torch.optim.Adam\n",
        "\n",
        "    def init(self):\n",
        "        # Create epsilon_theta(x_t, t) model\n",
        "        self.eps_model = UNet(\n",
        "            image_channels=self.image_channels,\n",
        "            n_channels=self.n_channels,\n",
        "            ch_mults=self.channel_multipliers,\n",
        "            is_attn=self.is_attention,\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Create DDPM class\n",
        "        self.diffusion = DenoiseDiffusion(\n",
        "            eps_model=self.eps_model,\n",
        "            n_steps=self.n_steps,\n",
        "            device=self.device,\n",
        "        )\n",
        "\n",
        "        # Create dataloader\n",
        "        self.data_loader = torch.utils.data.DataLoader(self.dataset, self.batch_size, shuffle=True, pin_memory=True)\n",
        "        # Create optimizer\n",
        "        self.optimizer = torch.optim.Adam(self.eps_model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "    def sample(self):\n",
        "        with torch.no_grad():\n",
        "            # [1]\n",
        "            x = torch.randn(self.n_samples, self.image_channels, self.image_size, self.image_size, device=self.device)\n",
        "\n",
        "            # Remove noise for T steps\n",
        "            progress_bar = tqdm(range(self.n_steps))\n",
        "            for t_ in progress_bar:\n",
        "                progress_bar.set_description(f\"Sampling\")\n",
        "                # t\n",
        "                t = self.n_steps - t_ - 1\n",
        "                # [2]\n",
        "                x = self.diffusion.p_sample(x, x.new_full((self.n_samples,), t, dtype=torch.long))\n",
        "\n",
        "            # Log samples\n",
        "            plot_samples(x.detach().cpu())\n",
        "\n",
        "    def train(self, epoch):\n",
        "        # Iterate through the dataset\n",
        "        progress_bar = tqdm(self.data_loader)\n",
        "        for data in progress_bar:\n",
        "            # Increment global step\n",
        "            progress_bar.set_description(f\"Epoch {epoch + 1}\")\n",
        "            # Move data to device\n",
        "            data = data.to(self.device)\n",
        "\n",
        "            # Make the gradients zero\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = self.diffusion.loss(data)\n",
        "            # Compute gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Take an optimization step\n",
        "            self.optimizer.step()\n",
        "\n",
        "            # Track the loss\n",
        "            progress_bar.set_postfix(loss=loss.detach().cpu().numpy())\n",
        "\n",
        "    def run(self):\n",
        "        for epoch in range(self.epochs):\n",
        "            # Train the model\n",
        "            self.train(epoch)\n",
        "            # Sample some images\n",
        "            self.sample()\n",
        "\n",
        "\n",
        "# Create configurations\n",
        "configs = Configs()\n",
        "\n",
        "# Initialize\n",
        "configs.init()\n",
        "\n",
        "# Start and run the training loop\n",
        "configs.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXu2a8G1Q4Vv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMMNVcUcLqpJt7XXtaaJitl",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
